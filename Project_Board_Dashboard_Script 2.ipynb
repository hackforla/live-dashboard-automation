{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af3d2122-0d9c-4863-ae9c-2c9bb8c95667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from urllib.parse import parse_qs, urlparse\n",
    "from datetime import datetime\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b382fd64-e752-494b-b4d0-298eb008fbd5",
   "metadata": {},
   "source": [
    "# Set Up for API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f814c1-eb2d-4e32-9121-79ba280e9898",
   "metadata": {},
   "outputs": [],
   "source": [
    "GitHub_token = os.environ[\"API_KEY_GITHUB_PROJECTBOARD_DASHBOARD\"]\n",
    "user = os.environ['API_TOKEN_USERNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93adeaaa-ba7f-4e4c-bd20-10429d53bfed",
   "metadata": {},
   "source": [
    "# Get Cards in Project Board Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d105a83-534f-480c-8a17-ddff9b59d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cards(url):\n",
    "    complete_df = pd.DataFrame()\n",
    "    \n",
    "    # use impossible number of 1000 (99900 issues in project board column) to ensure issues from all page results are retrieved\n",
    "    for i in range(1, 1000):\n",
    "        params = {\"per_page\": 100, \"page\": i}\n",
    "        response = requests.get(url, auth=(user, GitHub_token), params = params)\n",
    "        df = pd.DataFrame(response.json())\n",
    "        if len(df) > 0:\n",
    "            complete_df = pd.concat([complete_df, df], ignore_index = True)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    return complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc788cd5-ad5f-4e75-b279-c8979d953c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ice Box\n",
    "ice_box = get_cards('https://api.github.com/projects/columns/7198227/cards') \n",
    "\n",
    "# ER Column\n",
    "er = get_cards('https://api.github.com/projects/columns/19403960/cards') \n",
    "\n",
    "# New Issue Approval Column\n",
    "newissue_approval = get_cards('https://api.github.com/projects/columns/15235217/cards') \n",
    "\n",
    "# Prioritized Backlog Column\n",
    "prioritized_backlog = get_cards('https://api.github.com/projects/columns/7198257/cards') \n",
    "\n",
    "# \"In Progress (Actively Working)\" Column\n",
    "in_progress = get_cards('https://api.github.com/projects/columns/7198228/cards') \n",
    "\n",
    "# Questions/In Review Column\n",
    "questions = get_cards('https://api.github.com/projects/columns/8178690/cards') \n",
    "\n",
    "# QA Column\n",
    "QA = get_cards('https://api.github.com/projects/columns/15490305/cards') \n",
    "\n",
    "# UAT Column\n",
    "UAT = get_cards('https://api.github.com/projects/columns/17206624/cards') \n",
    "\n",
    "# \"QA - senior review\" Column\n",
    "QA_review = get_cards('https://api.github.com/projects/columns/19257634/cards') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09774ab-f927-4455-984c-5060dddf5c6b",
   "metadata": {},
   "source": [
    "# Prep Work: Create Variables with List of Main Complexity Labels and Status Breakdowns for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fabc5c-8b81-44f8-9173-e903fd9ad1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity_labels = [\"Complexity: Prework\", \"Complexity: Missing\", \"Complexity: Large\", \n",
    "                     \"Complexity: Extra Large\", \"Complexity: Small\", \"good first issue\", \n",
    "                     \"Complexity: Medium\", \"Complexity: See issue making label\", \"prework\", \n",
    "                     \"Complexity: Good second issue\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "736c4c9b-38d2-4f46-99af-b6831ac365ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_breakdown = [\"Draft\", \"2 weeks inactive\", \"ready for product\", \n",
    "                   \"ready for dev lead\", \"Ready for Prioritization\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b2361-aa00-4069-bdff-cd978ecf171a",
   "metadata": {},
   "source": [
    "# Get Issue Links from Cards in Project Board Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5dafb5a7-cc45-4e5a-b68f-3bf283c801f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for cleaning and getting datasets to reduce script length\n",
    "\n",
    "def cleaning(df_name):\n",
    "    issues = list(df_name[~df_name['content_url'].isna()]['content_url']) \n",
    "    issues_df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        for url in issues:\n",
    "            response = requests.get(url, auth=(user, GitHub_token))\n",
    "            issue_data = pd.json_normalize(response.json())\n",
    "            issues_df = pd.concat([issues_df, issue_data], ignore_index = True)\n",
    "    except ValueError:\n",
    "        time.sleep(3600)\n",
    "        for url in issues:\n",
    "            response = requests.get(url, auth=(user, GitHub_token))\n",
    "            issue_data = pd.json_normalize(response.json())\n",
    "            issues_df = pd.concat([issues_df, issue_data], ignore_index = True)\n",
    "    \n",
    "    # Get current time in LA\n",
    "    datetime_LA = datetime.now(pytz.timezone('US/Pacific') )\n",
    "    \n",
    "    # Format the time as a string and add it in Runtime column\n",
    "    issues_df[\"Runtime\"] = \"LA time: \" + datetime_LA.strftime(\"%m/%d/%Y %H:%M:%S\")\n",
    "    \n",
    "    # Drop unneccessary columns\n",
    "    drop_columns = [x for x in issues_df.columns if re.match(\"url|id|closed|state|assignee|user|reactions|milestone|number|locked|comments|created|updated|author_association|active_lock_reason|body|performed_via_github_app\", x)]\n",
    "    issues_df.drop(columns = drop_columns, inplace = True)\n",
    "    \n",
    "    return issues_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02bd9092-02bf-4ece-a9b9-b4be9a4762e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning2(df_name):\n",
    "    # Flatten labels column\n",
    "    flatten = df_name.to_json(orient = \"records\")\n",
    "    parsed = json.loads(flatten)\n",
    "    \n",
    "    # Drop redundant labels columns\n",
    "    issues_df2 = pd.json_normalize(parsed, record_path = [\"labels\"], record_prefix = \"labels.\", meta = [\"Runtime\", \"html_url\", \"title\"])\n",
    "    issues_df2.drop(columns = ['labels.id', 'labels.node_id', 'labels.url', 'labels.description', 'labels.color', 'labels.default'], inplace = True)\n",
    "    \n",
    "    # Remove issues with ignore labels in column\n",
    "    if len([label for label in issues_df2[\"labels.name\"].unique() if re.search('ignore', label.lower())])>0:\n",
    "        remove = list(issues_df2[issues_df2[\"labels.name\"].str.contains(\"gnore\")][\"html_url\"])\n",
    "        issues_df2 = issues_df2[~issues_df2[\"html_url\"].isin(remove)]\n",
    "    else:\n",
    "        remove = []\n",
    "        \n",
    "    # Finishing touches for icebox dataset (include issues with no labels)\n",
    "    difference = list(set(df_name[\"html_url\"]).difference(set(issues_df2[\"html_url\"])))\n",
    "    no_labels = list(set(difference).difference(set(remove)))\n",
    "    no_labels_df = df_name[df_name[\"html_url\"].isin(no_labels)][[\"Runtime\", \"html_url\", \"title\"]]\n",
    "    no_labels_df[\"labels.name\"] = \"\"\n",
    "    no_labels_df = no_labels_df[[\"labels.name\", \"Runtime\", \"html_url\", \"title\"]]\n",
    "\n",
    "    issues_df3 = pd.concat([issues_df2, no_labels_df], ignore_index = True)\n",
    "    \n",
    "    return issues_df2, issues_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9906cce7-f37d-4108-a180-fc37115843f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_cleaning(df_name):\n",
    "    # retain only labels with \"role\" in it or complexity labels, and \"Draft\", \"ready for product\", \"ready for prioritization\", \"ready for dev lead\"\n",
    "    final = df_name[(df_name[\"labels.name\"].str.contains(\"role\") | df_name[\"labels.name\"].isin(complexity_labels) | \n",
    "                              df_name[\"labels.name\"].isin(extra_breakdown) | df_name[\"labels.name\"].str.contains(\"Ready\", case=False))]\n",
    "\n",
    "    # Make combined label for issues with front and backend labels\n",
    "    wdataset = final[final[\"labels.name\"].str.contains(\"front end\") | final[\"labels.name\"].str.contains(\"back end\")]\n",
    "    wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
    "\n",
    "    final.loc[list(wdataset[wdataset[\"front/back end count\"] == 2].index), \"labels.name\"] = \"role: front end and backend/DevOps\"\n",
    "\n",
    "    final.drop_duplicates(inplace = True)\n",
    "\n",
    "    final2 = final[[\"Runtime\", \"labels.name\", \"html_url\", \"title\"]]\n",
    "    \n",
    "    return final2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8c5977d-c373-49b8-a249-e116935b6023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  wdataset[\"front/back end count\"] = wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\669818904.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final.drop_duplicates(inplace = True)\n"
     ]
    }
   ],
   "source": [
    "# Get Datasets for Icebox Column\n",
    "icebox_issues_df = cleaning(ice_box)\n",
    "icebox_issues_df2, icebox_issues_df3 = cleaning2(icebox_issues_df)\n",
    "icebox_issues_df3[\"Project Board Column\"] = \"1 - Icebox\"\n",
    "final_icebox2 = final_cleaning(icebox_issues_df2)\n",
    "\n",
    "# Get Datasets for Emergent Request Column\n",
    "ER_issues_df = cleaning(er)\n",
    "ER_issues_df2, ER_issues_df3 = cleaning2(ER_issues_df)\n",
    "ER_issues_df3[\"Project Board Column\"] = \"2- ER\"\n",
    "final_ER2 = final_cleaning(ER_issues_df2)\n",
    "\n",
    "# Get Datasets for New Issue Approval Column\n",
    "NIA_issues_df = cleaning(newissue_approval)\n",
    "NIA_issues_df2, NIA_issues_df3 = cleaning2(NIA_issues_df)\n",
    "NIA_issues_df3[\"Project Board Column\"] = \"3 - New Issue Approval\"\n",
    "final_NIA2 = final_cleaning(NIA_issues_df2)\n",
    "\n",
    "# Get Datasets for Prioritized Backlog Column\n",
    "pb_issues_df = cleaning(prioritized_backlog)\n",
    "pb_issues_df2, pb_issues_df3 = cleaning2(pb_issues_df)\n",
    "pb_issues_df3[\"Project Board Column\"] = \"4 - Prioritized Backlog\"\n",
    "final_pb2 = final_cleaning(pb_issues_df2)\n",
    "\n",
    "# Get Datasets for In Progress Column\n",
    "ip_df = cleaning(in_progress)\n",
    "ip_df2, ip_issues_df3 = cleaning2(ip_df)\n",
    "ip_issues_df3[\"Project Board Column\"] = \"5 - In Progress\"\n",
    "final_ip2 = final_cleaning(ip_df2)\n",
    "\n",
    "# Get Datasets for Questions/In Review Column\n",
    "questions_issues_df = cleaning(questions)\n",
    "questions_issues_df2, questions_issues_df3 = cleaning2(questions_issues_df)\n",
    "questions_issues_df3[\"Project Board Column\"] = \"6 - Questions/ In Review\"\n",
    "final_questions2 = final_cleaning(questions_issues_df2)\n",
    "\n",
    "# Get Datasets for QA Column\n",
    "QA_issues_df = cleaning(QA)\n",
    "QA_issues_df2, QA_issues_df3 = cleaning2(QA_issues_df)\n",
    "QA_issues_df3[\"Project Board Column\"] = \"7 - QA\"\n",
    "final_QA2 = final_cleaning(QA_issues_df2)\n",
    "\n",
    "# Get Datasets for UAT Column\n",
    "UAT_issues_df = cleaning(UAT)\n",
    "UAT_issues_df2, UAT_issues_df3 = cleaning2(UAT_issues_df)\n",
    "UAT_issues_df3[\"Project Board Column\"] = \"8 - UAT\"\n",
    "final_UAT2 = final_cleaning(UAT_issues_df2)\n",
    "\n",
    "# Get Datasets for QA - Senior Review Column\n",
    "QA_review_issues_df = cleaning(QA_review)\n",
    "QA_review_issues_df2, QA_review_issues_df3 = cleaning2(QA_review_issues_df)\n",
    "QA_review_issues_df3[\"Project Board Column\"] = \"9 - QA (senior review)\"\n",
    "final_QA_review2 = final_cleaning(QA_review_issues_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19764d72-77dc-44dd-a9ca-5335053d1d1b",
   "metadata": {},
   "source": [
    "# Create Data Source for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7801c3af-a4e8-402f-b979-bef360db6ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "from googleapiclient.discovery import build\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "\n",
    "import gspread\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "167a2a56-0596-489e-bc90-3b5c7a328f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scopes = ['https://www.googleapis.com/auth/spreadsheets',\n",
    "          'https://www.googleapis.com/auth/drive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a9aa30f-b545-4c24-a780-7db2dd963ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in offical GitHub labels from Google spreadsheet for weekly label check table\n",
    "\n",
    "key_base64 = = os.environ[\"BASE64_PROJECT_BOARD_GOOGLECREDENTIAL\"]\n",
    "base64_bytes = key_base64.encode('ascii')\n",
    "key_base64_bytes = base64.b64decode(base64_bytes)\n",
    "key_content = key_base64_bytes.decode('ascii')\n",
    "\n",
    "service_account_info = json.loads(key_content)\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_info(service_account_info, scopes = scopes)\n",
    "\n",
    "service_sheets = build('sheets', 'v4', credentials = credentials)\n",
    "\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "gauth = GoogleAuth()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "\n",
    "LabelCheck_GOOGLE_SHEETS_ID = '1-ltg0qMeZSgOnqrCU0nKUDQd1JOXTMWrNTK63VZjXdk'\n",
    "\n",
    "LabelCheck_sheet_name = 'Official GitHub Labels'\n",
    "\n",
    "gs = gc.open_by_key(LabelCheck_GOOGLE_SHEETS_ID)\n",
    "\n",
    "LabelCheck_worksheet = gs.worksheet(LabelCheck_sheet_name)\n",
    "\n",
    "LC_spreadsheet_data = LabelCheck_worksheet.get_all_records()\n",
    "LC_df = pd.DataFrame.from_dict(LC_spreadsheet_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a54ed-1edb-4fc7-a7fb-6582ab7e6289",
   "metadata": {},
   "source": [
    "## Create Functions for Data Manipulation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74661a-0a72-436e-b31f-891113fda4b4",
   "metadata": {},
   "source": [
    "### Add Columns for Counting Number of Issues with Specific Labels Indicating Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec1102b-7e73-41fc-a494-9aab7083cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to add:\n",
    "# 1) Draft, 2) 2 weeks inactive, 3) ready for product, 4) ready for dev lead, 5) Ready for Prioritization\n",
    "\n",
    "def add_known_status_col(df_name, project_board_column):\n",
    "    role = df_name[df_name[\"labels.name\"].str.contains(\"role\")]\n",
    "    complexity = df_name[df_name[\"labels.name\"].isin(complexity_labels)]\n",
    "\n",
    "    dataset = role.merge(complexity, how = \"outer\", on = [\"html_url\", \"title\"])\n",
    "    dataset.rename(columns = {\"labels.name_x\": \"Role Label\", \"labels.name_y\": \"Complexity Label\", \"Runtime_x\":\"Runtime\"}, inplace = True)\n",
    "\n",
    "    runtime_nulls_loc = dataset[dataset[\"Runtime\"].isna()].index\n",
    "    dataset.loc[runtime_nulls_loc, \"Runtime\"]= dataset[~dataset[\"Runtime\"].isna()].iloc[0,0]\n",
    "    dataset.drop(columns = [\"Runtime_y\"], inplace = True)\n",
    "\n",
    "    for label in extra_breakdown:\n",
    "        if len(df_name[df_name[\"labels.name\"]==label]) > 0:\n",
    "            col_label = df_name[df_name[\"labels.name\"]==label][[\"html_url\", \"title\", \"labels.name\"]]\n",
    "            dataset = dataset.merge(col_label, how = \"left\", on = [\"html_url\", \"title\"])\n",
    "            dataset[\"labels.name\"] = dataset[\"labels.name\"].map(lambda x: 1 if x == label else 0)\n",
    "            dataset.rename(columns = {\"labels.name\": label}, inplace = True)\n",
    "        elif len(df_name[df_name[\"labels.name\"]==label]) == 0:\n",
    "            dataset[label] = 0\n",
    "            \n",
    "    dataset[\"Project Board Column\"] = project_board_column\n",
    "    \n",
    "    dataset2 = dataset[[\"Project Board Column\", \"Runtime\", \"Role Label\", \"Complexity Label\", \"html_url\", \"title\", \"Draft\", \"2 weeks inactive\", \"ready for product\", \"ready for dev lead\", \"Ready for Prioritization\"]]\n",
    "        \n",
    "    return dataset2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781bc1c0-d9e1-4623-9f51-6c9781374a8c",
   "metadata": {},
   "source": [
    "### Add Column for Counting Number of Issues with Unknown Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c24272ed-dbdd-4cd6-a1b5-c89983c650c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unknown_status_col(copy_df, df_name, known_status_regex):\n",
    "    # Create a column to identify issues with unknown status\n",
    "    unknown_status_wdataset = copy_df.copy()\n",
    "    unknown_status_wdataset[\"Known Status\"] = unknown_status_wdataset[\"labels.name\"].map(lambda x: 1 if (re.search(known_status_regex, str(x).lower())) else 0)\n",
    "    known_status_issues = list(unknown_status_wdataset[unknown_status_wdataset[\"Known Status\"] == 1][\"html_url\"].unique())\n",
    "    df_name[\"Unknown Status\"] = df_name[\"html_url\"].map(lambda x: 0 if x in known_status_issues else 1)\n",
    "    \n",
    "    return df_name\n",
    "\n",
    "# r\"(ready|draft|^dependency$)\" for icebox\n",
    "# r\"(ready|draft)\" for all other columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb2c4f3-d8d8-45a4-9c2b-90ba4fcfe8cb",
   "metadata": {},
   "source": [
    "### Add Column with Link to Issues with Unknown Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da9aeed0-fda1-458b-8fcd-e0cf0633686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all ready series labels and add them to the status link\n",
    "ready_labels = list(LC_df[LC_df[\"label_series\"] == \"ready\"][\"label_name\"].unique())\n",
    "ready_labels_append = \"\"\n",
    "for label in ready_labels:\n",
    "    ready_labels_transformed = label.lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "    ready_labels_append = ready_labels_append+\"+-label%3A%22\"+ ready_labels_transformed+\"%22\"\n",
    "\n",
    "# Transform all ignore series labels and add them to the status link\n",
    "ignore_labels = list(LC_df[LC_df[\"label_series\"] == \"ignore\"][\"label_name\"].unique())\n",
    "ignore_labels_append = \"\"\n",
    "for label in ignore_labels:\n",
    "    ignore_labels_transformed = label.lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "    ignore_labels_append = ignore_labels_append+\"+-label%3A%22\"+ ignore_labels_transformed+\"%22\"\n",
    "        \n",
    "transformed_readyignorelabels = ready_labels_append + ignore_labels_append\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47772826-c87f-48ae-941f-e6fbb630475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "icebox_static_link_base = 'https://github.com/hackforla/website/projects/7?card_filter_query=-label%3Adraft%22+-label%3A%22dependency%22'+ transformed_readyignorelabels\n",
    "other_static_link_base = 'https://github.com/hackforla/website/projects/7?card_filter_query=-label%3Adraft' + transformed_readyignorelabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e82d8d61-e47d-460a-b282-e62981102718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested dictionary for static links\n",
    "\n",
    "def add_unknownstatus_link(base_link, df_name):\n",
    "    unique_roles = [x for x in df_name[\"Role Label\"].unique() if pd.isna(x) == False]\n",
    "    unique_roles2 = [x for x in unique_roles if x != \"role: front end and backend/DevOps\"]\n",
    "    unique_complexity = [x for x in df_name[\"Complexity Label\"].unique() if pd.isna(x) == False]  \n",
    "\n",
    "    link_dict = { }\n",
    "\n",
    "    for role in unique_roles2:\n",
    "        link_dict[role] = {}\n",
    "        for complexity in unique_complexity:\n",
    "            role_transformed = role.lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "            complexity_transformed = complexity.lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "            link_dict[role][complexity] = base_link+\"+label%3A%22\"+role_transformed+\"%22\"+\"+label%3A%22\"+complexity_transformed+\"%22\"\n",
    "\n",
    "    link_dict[\"role: front end and backend/DevOps\"] = {}     \n",
    "    for complexity in unique_complexity:\n",
    "        frontend_transformed = \"role: front end\".lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "        backend_transformed = \"role: back end/devOps\".lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "        complexity_transformed = complexity.lower().replace(\":\", \"%3A\").replace(\" \", \"+\")\n",
    "        link_dict[\"role: front end and backend/DevOps\"][complexity] = base_link+\"+label%3A%22\"+frontend_transformed+\"%22\"+\"+label%3A%22\"+backend_transformed+\"%22\"+\"+label%3A%22\"+complexity_transformed+\"%22\"\n",
    "\n",
    "    df_name[\"Role-based Link for Unknown Status\"] = \"\"\n",
    "    for role in link_dict.keys():\n",
    "        df = df_name[df_name[\"Role Label\"] == role]\n",
    "        for complexity in link_dict[list(link_dict.keys())[0]].keys(): #same for all roles\n",
    "            df2 = df[df[\"Complexity Label\"] == complexity]\n",
    "            indexes = df2.index\n",
    "            df_name.loc[indexes, \"Role-based Link for Unknown Status\"] = link_dict[role][complexity]\n",
    "            \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afd803-e54f-47eb-acaa-e9382e71eeb6",
   "metadata": {},
   "source": [
    "### Apply the Functions to Each Project Board Column Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7b3f65-fb8b-4fda-9f96-96090868e84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\3833982490.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name[\"Unknown Status\"] = df_name[\"html_url\"].map(lambda x: 0 if x in known_status_issues else 1)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\1244404064.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_name[\"Role-based Link for Unknown Status\"] = \"\"\n"
     ]
    }
   ],
   "source": [
    "# Icebox\n",
    "icebox_dataset2 = add_known_status_col(final_icebox2, \"1 - Icebox\")\n",
    "icebox_dataset2 = add_unknown_status_col(icebox_issues_df2, icebox_dataset2, r\"(ready|draft|^dependency$)\")\n",
    "icebox_dataset2 = add_unknownstatus_link(icebox_static_link_base, icebox_dataset2)\n",
    "\n",
    "# Emergent Request\n",
    "ER_dataset2 = add_known_status_col(final_ER2, \"2 - ER\")\n",
    "ER_dataset2 = add_unknown_status_col(ER_issues_df2, ER_dataset2, r\"(ready|draft)\")\n",
    "ER_dataset2 = add_unknownstatus_link(other_static_link_base, ER_dataset2)\n",
    "\n",
    "# New Issue Approval\n",
    "NIA_dataset2 = add_known_status_col(final_NIA2, \"3 - New Issue Approval\")\n",
    "NIA_dataset2 = add_unknown_status_col(NIA_issues_df2, NIA_dataset2, r\"(ready|draft)\")\n",
    "NIA_dataset2 = add_unknownstatus_link(other_static_link_base, NIA_dataset2)\n",
    "\n",
    "# Prioritized Backlog\n",
    "dataset2 = add_known_status_col(final_pb2, \"4 - Prioritized Backlog\")\n",
    "pb_dataset2 = add_unknown_status_col(pb_issues_df2, pb_dataset2, r\"(ready|draft)\")\n",
    "pb_dataset2 = add_unknownstatus_link(other_static_link_base, pb_dataset2)\n",
    "\n",
    "# In Progress\n",
    "IP_dataset2 = add_known_status_col(final_ip2, \"5 - In Progress\")\n",
    "IP_dataset2 = add_unknown_status_col(ip_df2, IP_dataset2, r\"(ready|draft)\")\n",
    "IP_dataset2 = add_unknownstatus_link(other_static_link_base, IP_dataset2)\n",
    "\n",
    "# Questions/In Review\n",
    "Q_dataset2 = add_known_status_col(final_questions2, \"6 - Questions/ In Review\")\n",
    "Q_dataset2 = add_unknown_status_col(questions_issues_df2, Q_dataset2, r\"(ready|draft)\")\n",
    "Q_dataset2 = add_unknownstatus_link(other_static_link_base, Q_dataset2)\n",
    "\n",
    "# QA\n",
    "QA_dataset2 = add_known_status_col(final_QA2, \"7 - QA\")\n",
    "QA_dataset2 = add_unknown_status_col(QA_issues_df2, QA_dataset2, r\"(ready|draft)\")\n",
    "QA_dataset2 = add_unknownstatus_link(other_static_link_base, QA_dataset2)\n",
    "\n",
    "# UAT\n",
    "UAT_dataset2 = add_known_status_col(final_UAT2, \"8 - UAT\")\n",
    "UAT_dataset2 = add_unknown_status_col(UAT_issues_df2, UAT_dataset2, r\"(ready|draft)\")\n",
    "UAT_dataset2 = add_unknownstatus_link(other_static_link_base, UAT_dataset2)\n",
    "\n",
    "# QA - senior review\n",
    "QA_review_dataset2 = add_known_status_col(final_QA_review2, \"9 - QA (senior review)\")\n",
    "QA_review_dataset2 = add_unknown_status_col(QA_review_issues_df2, QA_review_dataset2, r\"(ready|draft)\")\n",
    "QA_review_dataset2 = add_unknownstatus_link(other_static_link_base, QA_review_dataset2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a1ff2-d57c-40ca-ad4d-3f5a01d8da50",
   "metadata": {},
   "source": [
    "## Combine Data from All Project Board Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "077abba8-f693-4d71-ab17-5323b21fd75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the dataset and see whether dashboard would work\n",
    "final_dataset = pd.concat([icebox_dataset2, ER_dataset2, NIA_dataset2, pb_dataset2, IP_dataset2, Q_dataset2, QA_dataset2, UAT_dataset2, QA_review_dataset2], ignore_index = True)\n",
    "\n",
    "final_dataset.loc[final_dataset[final_dataset[\"Complexity Label\"] == \"good first issue\"].index, \"Complexity Label\"] = \"1 - good first issue\"\n",
    "final_dataset.loc[final_dataset[final_dataset[\"Complexity Label\"] == \"Complexity: Small\"].index, \"Complexity Label\"] = \"2 - Complexity: Small\"\n",
    "final_dataset.loc[final_dataset[final_dataset[\"Complexity Label\"] == \"Complexity: Medium\"].index, \"Complexity Label\"] = \"3 - Complexity: Medium\"\n",
    "final_dataset.loc[final_dataset[final_dataset[\"Complexity Label\"] == \"Complexity: Large\"].index, \"Complexity Label\"] = \"4 - Complexity: Large\"\n",
    "final_dataset.loc[final_dataset[final_dataset[\"Complexity Label\"] == \"Complexity: Extra Large\"].index, \"Complexity Label\"] = \"5 - Complexity: Extra Large\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe142aee-1531-4d47-99be-5800f13727e0",
   "metadata": {},
   "source": [
    "## Create Anomaly Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b79fd284-b32b-43f4-8890-451634f9fe53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df.drop(columns = [\"keep\"], inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df[\"labels_need_action\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if x not in official_active_labels else 0)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df[\"outdated_label\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if x in outdated_labels else 0)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df[\"unknown_label\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if (x not in official_active_labels and x not in outdated_labels) else 0)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df[\"Label Transformed\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: x.lower().replace(\":\", \"%3A\").replace(\" \", \"+\") if pd.isna(x) == False else x)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df[\"Link for Quick Correction\"] = anomaly_detection_df[\"Label Transformed\"].map(lambda x: \"https://github.com/hackforla/website/issues?q=is%3Aissue+label%3A\"+str(x) if pd.isna(x) == False else np.nan)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_df.drop(columns = [\"Label Transformed\"], inplace = True)\n",
      "C:\\Users\\kimbe\\AppData\\Local\\Temp\\ipykernel_41260\\4154204007.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  anomaly_detection_wdataset[\"front/back end count\"] = anomaly_detection_wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n"
     ]
    }
   ],
   "source": [
    "# Concat the dataframes from all columns\n",
    "\n",
    "anomaly_detection = pd.concat([icebox_issues_df3, ER_issues_df3, NIA_issues_df3, pb_issues_df3, ip_issues_df3, questions_issues_df3, QA_issues_df3, UAT_issues_df3, QA_review_issues_df3], ignore_index = True)\n",
    "anomaly_detection[\"keep\"] = anomaly_detection[\"labels.name\"].map(lambda x: 1 if (re.search(r\"(size|feature|role|complexity|good first issue|prework|^$)\", str(x).lower())) else 0)\n",
    "anomaly_detection_df = anomaly_detection[anomaly_detection[\"keep\"] == 1]\n",
    "anomaly_detection_df.drop(columns = [\"keep\"], inplace = True)\n",
    "\n",
    "outdated_labels = list(LC_df[LC_df[\"in_use?\"] == \"No\"][\"label_name\"].unique())\n",
    "official_active_labels = list(set(list(LC_df[\"label_name\"])).difference(set(outdated_labels)))\n",
    "\n",
    "anomaly_detection_df[\"labels_need_action\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if x not in official_active_labels else 0)\n",
    "anomaly_detection_df[\"outdated_label\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if x in outdated_labels else 0)\n",
    "anomaly_detection_df[\"unknown_label\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: 1 if (x not in official_active_labels and x not in outdated_labels) else 0)\n",
    "anomaly_detection_df[\"Label Transformed\"] = anomaly_detection_df[\"labels.name\"].map(lambda x: x.lower().replace(\":\", \"%3A\").replace(\" \", \"+\") if pd.isna(x) == False else x)\n",
    "anomaly_detection_df[\"Link for Quick Correction\"] = anomaly_detection_df[\"Label Transformed\"].map(lambda x: \"https://github.com/hackforla/website/issues?q=is%3Aissue+label%3A\"+str(x) if pd.isna(x) == False else np.nan)\n",
    "\n",
    "anomaly_detection_df.drop(columns = [\"Label Transformed\"], inplace = True)\n",
    "\n",
    "anomaly_detection_df = anomaly_detection_df[[\"Project Board Column\", \"Runtime\", \"html_url\", \"title\", \"labels.name\", \"labels_need_action\", \"outdated_label\", \"unknown_label\", \"Link for Quick Correction\"]]\n",
    "\n",
    "# In[49]:\n",
    "\n",
    "anomaly_detection_df2_base = anomaly_detection_df.copy()\n",
    "anomaly_detection_df2_base.drop(columns = [\"labels_need_action\"], inplace = True)\n",
    "anomaly_detection_df2_base.drop(columns = [\"outdated_label\"], inplace = True)\n",
    "anomaly_detection_df2_base.drop(columns = [\"unknown_label\"], inplace = True)\n",
    "anomaly_detection_df2_base.drop(columns = [\"Link for Quick Correction\"], inplace = True)\n",
    "\n",
    "# Includes official labels that are current and outdated\n",
    "official_complexity = list(LC_df[LC_df[\"label_series\"] == \"complexity\"][\"label_name\"])\n",
    "official_feature = list(LC_df[LC_df[\"label_series\"] == \"feature\"][\"label_name\"])\n",
    "official_role = list(LC_df[LC_df[\"label_series\"] == \"role\"][\"label_name\"])\n",
    "official_size = list(LC_df[LC_df[\"label_series\"] == \"size\"][\"label_name\"])\n",
    "\n",
    "anomaly_detection_df2_base[\"Complexity Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x in official_complexity else 0)\n",
    "anomaly_detection_df2_base[\"Feature Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x in official_feature else 0)\n",
    "anomaly_detection_df2_base[\"Role Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x in official_role else 0)\n",
    "anomaly_detection_df2_base[\"Size Label\"]= anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x in official_size else 0)\n",
    "\n",
    "complexity_missing_series = list(LC_df[(LC_df[\"label_series\"] == \"complexity\") & (LC_df[\"missing_series?\"] == \"Yes\")][\"label_name\"])[0]\n",
    "feature_missing_series = list(LC_df[(LC_df[\"label_series\"] == \"feature\") & (LC_df[\"missing_series?\"] == \"Yes\")][\"label_name\"])[0]\n",
    "role_missing_series = list(LC_df[(LC_df[\"label_series\"] == \"role\") & (LC_df[\"missing_series?\"] == \"Yes\")][\"label_name\"])[0]\n",
    "size_missing_series = list(LC_df[(LC_df[\"label_series\"] == \"size\") & (LC_df[\"missing_series?\"] == \"Yes\")][\"label_name\"])[0]\n",
    "\n",
    "anomaly_detection_df2_base[\"Complexity Missing Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x == complexity_missing_series else 0)\n",
    "anomaly_detection_df2_base[\"Feature Missing Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x == feature_missing_series else 0)\n",
    "anomaly_detection_df2_base[\"Role Missing Label\"] = anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x == role_missing_series else 0)\n",
    "anomaly_detection_df2_base[\"Size Missing Label\"]= anomaly_detection_df2_base[\"labels.name\"].map(lambda x: 1 if x == size_missing_series else 0)\n",
    " \n",
    "anomaly_detection_df2 = anomaly_detection_df2_base.groupby([\"Project Board Column\", \"Runtime\", \"html_url\", \"title\"])[[\"Complexity Label\", \"Feature Label\", \"Role Label\", \"Size Label\", \"Complexity Missing Label\", \"Feature Missing Label\", \"Role Missing Label\", \"Size Missing Label\"]].sum().reset_index()\n",
    "\n",
    "anomaly_detection_df2[\"Complexity defined label\"] = anomaly_detection_df2[\"Complexity Label\"]-anomaly_detection_df2[\"Complexity Missing Label\"]\n",
    "anomaly_detection_df2[\"Feature defined label\"] = anomaly_detection_df2[\"Feature Label\"]-anomaly_detection_df2[\"Feature Missing Label\"]\n",
    "anomaly_detection_df2[\"Role defined label\"] = anomaly_detection_df2[\"Role Label\"]-anomaly_detection_df2[\"Role Missing Label\"]\n",
    "anomaly_detection_df2[\"Size defined label\"] = anomaly_detection_df2[\"Size Label\"]-anomaly_detection_df2[\"Size Missing Label\"]\n",
    "\n",
    "anomaly_detection_df2_join = anomaly_detection_df2_base[anomaly_detection_df2_base[\"Role Label\"] == 1][[\"html_url\", \"labels.name\"]]\n",
    "anomaly_detection_df2 = anomaly_detection_df2.merge(anomaly_detection_df2_join, how = \"left\", on = [\"html_url\"])\n",
    "epic_issues = list(anomaly_detection[anomaly_detection['labels.name'] == 'epic']['html_url'].unique())\n",
    "ER_issues = list(anomaly_detection[anomaly_detection['labels.name'] == 'ER']['html_url'].unique())\n",
    "anomaly_detection_df2['Epic Issue?'] = anomaly_detection_df2['html_url'].map(lambda x: 1 if x in epic_issues else 0)\n",
    "anomaly_detection_df2['ER Issue?'] = anomaly_detection_df2['html_url'].map(lambda x: 1 if x in ER_issues else 0)\n",
    "\n",
    "# change role label of issues with front end and back end labels\n",
    "anomaly_detection_wdataset = anomaly_detection_df2[anomaly_detection_df2[\"labels.name\"].str.contains(\"front end\") | anomaly_detection_df2[\"labels.name\"].str.contains(\"back end\")]\n",
    "anomaly_detection_wdataset[\"front/back end count\"] = anomaly_detection_wdataset.groupby([\"html_url\", \"title\"])[\"labels.name\"].transform(\"count\")\n",
    "\n",
    "anomaly_detection_df2.loc[list(anomaly_detection_wdataset[anomaly_detection_wdataset[\"front/back end count\"] == 2].index), \"labels.name\"] = \"role: front end and backend/DevOps\"\n",
    "\n",
    "# Drop duplicates that have been created by the change\n",
    "anomaly_detection_df2.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d3f78-ca07-4a22-b68f-c584bee8d335",
   "metadata": {},
   "source": [
    "## Create dataset that detects issues with missing dependencies in icebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68508988-d986-4291-a451-2549b6acbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dependency_label = list(LC_df[(LC_df[\"label_series\"] == \"dependency\") & (LC_df[\"missing_series?\"] == \"Yes\")][\"label_name\"])[0]\n",
    "missing_dependency = anomaly_detection[(anomaly_detection[\"labels.name\"] == missing_dependency_label) & (anomaly_detection[\"Project Board Column\"] == \"1 - Icebox\")]\n",
    "icebox_issues = list(anomaly_detection[anomaly_detection[\"Project Board Column\"] == \"1 - Icebox\"][\"html_url\"].unique())\n",
    "icebox_issues_with_dependency = list(anomaly_detection[(anomaly_detection[\"Project Board Column\"] == \"1 - Icebox\") & (anomaly_detection[\"labels.name\"] == \"Dependency\")][\"html_url\"].unique())\n",
    "icebox_issues_without_dependency = list(set(icebox_issues).difference(set(icebox_issues_with_dependency)))\n",
    "no_dependency = anomaly_detection[anomaly_detection[\"html_url\"].isin(icebox_issues_without_dependency)]\n",
    "missing_dependency = pd.concat([missing_dependency, no_dependency], ignore_index = True)\n",
    "\n",
    "missing_dependency = missing_dependency.iloc[:, [1,4,2,3,0]]\n",
    "\n",
    "if len(missing_dependency) == 0:\n",
    "    missing_dependency.loc[0] = [\" \",\" \",\" \",\" \",\" \"]\n",
    "else:\n",
    "    missing_dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5de5789-7c45-434d-8524-486a72f5f997",
   "metadata": {},
   "source": [
    "## Create dataset with issues that have labels in missing series (to be joined for anomaly report in Looker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb986430-0283-46e5-9338-34bbb7c68d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "missingseries_labels = list(LC_df[LC_df[\"missing_series?\"] == \"Yes\"][\"label_name\"])\n",
    "issues_w_missinglabels = anomaly_detection[anomaly_detection['labels.name'].isin(missingseries_labels)][[\"Project Board Column\", \"html_url\", \"title\", \"labels.name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d05d70-928d-4157-8488-d09881f44a19",
   "metadata": {},
   "source": [
    "## Create new table that draws in all issues with ER title that do not have ER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c980bc2f-ea7d-4c23-8f12-0e91a8ee736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ER_label_check = ER_issues_df3.copy()\n",
    "ER_label_check[\"ER Label?\"] = ER_label_check['html_url'].map(lambda x: 1 if x in ER_issues else 0)\n",
    "No_ER_label = ER_label_check[ER_label_check[\"ER Label?\"] == 0]\n",
    "No_ER_label_filtered = No_ER_label[~No_ER_label[\"title\"].str.contains(\"ER from TLDL\", case = False)]\n",
    "No_ER_label_filtered.drop(columns = [\"labels.name\"], inplace = True)\n",
    "No_ER_label_filtered.drop_duplicates(inplace = True)\n",
    "\n",
    "if len(No_ER_label_filtered) == 0:\n",
    "    No_ER_label_filtered = pd.DataFrame(columns = [\"Runtime\", \"html_url\", \"title\", \"Project Board Column\", \"ER Label?\", \"state\"])\n",
    "    No_ER_label_filtered.loc[0] = [\" \",\" \",\" \",\" \",\" \",\" \"]\n",
    "else:\n",
    "    no_ERlabel_issuestate = pd.DataFrame()\n",
    "\n",
    "    for url in No_ER_label_filtered[\"html_url\"]:\n",
    "        issue_number = re.findall(r'[0-9]+$', url)[0]\n",
    "        html = \"https://api.github.com/repos/hackforla/website/issues/\"+issue_number\n",
    "        response = requests.get(html, auth=(user, GitHub_token))\n",
    "        df = pd.json_normalize(response.json())[[\"html_url\", \"state\"]]\n",
    "        no_ERlabel_issuestate = pd.concat([no_ERlabel_issuestate, df], ignore_index = True)\n",
    "\n",
    "    No_ER_label_filtered = No_ER_label_filtered.merge(no_ERlabel_issuestate, how = \"left\", on = \"html_url\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdedaff-162b-458c-8ff8-939a56d51049",
   "metadata": {},
   "source": [
    "## Create a table that displays issues with Complexity: Missing label with first comment being an empty description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e3f23f6-26dd-4c17-85fd-7ccfb7c400a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_columns = [\"1 - Icebox\", \"2 - ER\", \"3 - New Issue Approval\"]\n",
    "empty_description_search = final_dataset[(~final_dataset[\"Project Board Column\"].isin(excluded_columns)) & (final_dataset[\"Complexity Label\"] == \"Complexity: Missing\")]\n",
    "\n",
    "empty_comment = []\n",
    "\n",
    "for url in empty_description_search[\"html_url\"]:\n",
    "    issue_number = re.findall(r'[0-9]+$', url)[0]\n",
    "    html = \"https://api.github.com/repos/hackforla/website/issues/\"+issue_number+\"/timeline\"\n",
    "    response = requests.get(html, auth=(user, GitHub_token))\n",
    "    df = pd.DataFrame(response.json())\n",
    "    if (\"body\" not in list(df.columns)):\n",
    "        if (df.iloc[0][\"actor\"]['login'] != 'github-actions[bot]' and df.iloc[0][\"event\"] == \"cross-referenced\"):\n",
    "            empty_comment.append(url)\n",
    "    elif (\"body\" in list(df.columns)):\n",
    "        if (pd.isna(df.iloc[0][\"body\"]) == True and df.iloc[0][\"actor\"]['login'] != 'github-actions[bot]' and df.iloc[0][\"event\"] == \"cross-referenced\"):\n",
    "            empty_comment.append(url)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "complexity_missing_emptycomment = final_dataset[final_dataset[\"html_url\"].isin(empty_comment)][[\"Project Board Column\", \"Role Label\", \"Complexity Label\", \"html_url\", \"title\"]]\n",
    "if len(complexity_missing_emptycomment) == 0:\n",
    "    complexity_missing_emptycomment.loc[0] = [\" \",\" \",\" \",\" \",\" \"]\n",
    "else:\n",
    "    complexity_missing_emptycomment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb85886-370e-40f0-a529-9ddb896e4f09",
   "metadata": {},
   "source": [
    "# Send Data to Google Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a74b659-606c-4143-acdd-b88587928bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Send to Google Sheet\n",
    "\n",
    "Main_GOOGLE_SHEETS_ID = '1aJ0yHkXYMWTtMz6eEeolTLmAQOBc2DyptmR5SAmUrjM'\n",
    "\n",
    "sheet_name1 = 'Dataset 2'\n",
    "\n",
    "gs = gc.open_by_key(Main_GOOGLE_SHEETS_ID)\n",
    "\n",
    "worksheet1 = gs.worksheet(sheet_name1)\n",
    "\n",
    "worksheet1.clear()\n",
    "\n",
    "# Insert dataframe of issues into Google Sheet\n",
    "\n",
    "set_with_dataframe(worksheet = worksheet1, dataframe = final_dataset, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name2 = 'Labels to note'\n",
    "worksheet2 = gs.worksheet(sheet_name2)\n",
    "worksheet2.clear()\n",
    "set_with_dataframe(worksheet = worksheet2, dataframe = anomaly_detection_df, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name3 = 'Missing Labels'\n",
    "worksheet3 = gs.worksheet(sheet_name3)\n",
    "worksheet3.clear()\n",
    "set_with_dataframe(worksheet = worksheet3, dataframe = anomaly_detection_df2, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name4 = 'Issues with Missing Series Labels'\n",
    "worksheet4 = gs.worksheet(sheet_name4)\n",
    "worksheet4.clear()\n",
    "set_with_dataframe(worksheet = worksheet4, dataframe = issues_w_missinglabels, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name5 = 'Icebox Issues with Missing or No Dependency'\n",
    "worksheet5 = gs.worksheet(sheet_name5)\n",
    "worksheet5.clear()\n",
    "set_with_dataframe(worksheet = worksheet5, dataframe = missing_dependency, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name6 = 'Missing ER Label'\n",
    "worksheet6 = gs.worksheet(sheet_name6)\n",
    "worksheet6.clear()\n",
    "set_with_dataframe(worksheet = worksheet6, dataframe = No_ER_label_filtered, include_index = False, include_column_header = True, resize = True)\n",
    "\n",
    "sheet_name7 = 'Complexity Missing Issues with Empty 1st Comment'\n",
    "worksheet7 = gs.worksheet(sheet_name7)\n",
    "worksheet7.clear()\n",
    "set_with_dataframe(worksheet = worksheet7, dataframe = complexity_missing_emptycomment, include_index = False, include_column_header = True, resize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d3fb26-2f14-4395-8852-d36688a47d86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
